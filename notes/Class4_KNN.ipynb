{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__K Neareast Neighbors__\n",
      "\n",
      "NON-PARAMETRIC ALGO\n",
      "LOCAL HYPOTHESIS\n",
      "NON-GENERALIZING ALGO\n",
      "LAZY LEARNER\n",
      "\n",
      "Histograms are non parametric. Also rank.\n",
      "\n",
      "Parametric vs. non-parametric\n",
      "\n",
      "non-parametric closely related to clustering\n",
      "\n",
      "Training step: just save training records\n",
      "Test step: for pts in test set, find k nearest neighbors in training set, and form a prediction (eg majority vote)\n",
      "Sort of making a 'local hypothesis'\n",
      "\n",
      "No computation complexity in training, but as a result a lot more complexity in the testing phase.\n",
      "\n",
      "Cross validation you get for free due to lazy learning\n",
      "\n",
      "Can parallelize this or use kdtrees\n",
      "\n",
      "Try to use odd numbers for K so there aren't ties\n",
      "\n",
      "Need to define metric to measure distance. \n",
      "\n",
      "Examples of metrics:\n",
      "\n",
      "* Jaccard coefficient\n",
      "\n",
      "* Cosine similarity\n",
      "\n",
      "* Levenstein distance\n",
      "\n",
      "* Hamming distance\n",
      "\n",
      "* TF-IDF\n",
      "\n",
      "By making a metric, you are embedding feature into metric space\n",
      "\n",
      "If features are numbers, you are obviously already done, just use euclidian distance\n",
      "\n",
      "Taxi/manahttan cab distance, or maybe knight on chess board. What is shortest distance between pts if you are on a grid.\n",
      "\n",
      "(|x-y|^p)^1/p\n",
      "\n",
      "So you need to find the right metrics, and the result will depend on the metric\n",
      "\n",
      "So what can we say about the hypothesis space a KNN can achieve?\n",
      "\n",
      "Even though we are making local hypotheses, there will be an implicit model of the entire scheme\n",
      "\n",
      "Could be infinite complexity, need infinte pts and 1 for K in order to make it shatter.\n",
      "\n",
      "A VC dimention of 1 nearest neighbors is infinity\n",
      "\n",
      "Voronoi tesselation illustrates decision boundary (defines cells around training pts)\n",
      "\n",
      "for dimensions = d, VC(L) = 1 + d\n",
      "\n",
      "very easy to overfit with KNN, but still possible to underfit\n",
      "\n",
      "confusion matrix is nice because there are no statistics!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}